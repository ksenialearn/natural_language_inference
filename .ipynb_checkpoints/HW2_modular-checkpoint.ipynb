{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens, embedding, max_vocab_size = 10000):\n",
    "\n",
    "    # save index 1 for unk and 0 for pad\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    all_tokens = [item for sublist in all_tokens for item in sublist]\n",
    "    max_len = max([len(word) for word in all_tokens])\n",
    "    \n",
    "    unique_words = list(embedding.keys())\n",
    "    \n",
    "    id2token =  unique_words #list of words available in embedding\n",
    "    id2token = ['<pad>', '<unk>'] + id2token #add pad and unknown to the beginning\n",
    "    \n",
    "    token2id = dict(zip(unique_words, range(2,2+len(unique_words)))) # dictionary of words and indices \n",
    "    token2id['<pad>'] = PAD_IDX  #add pad symbol to the dictionary\n",
    "    token2id['<unk>'] = UNK_IDX  #add unkown symbol to the dictionary\n",
    "    \n",
    "    return token2id, id2token, max_len\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [list(token2id[token]) if token in token2id else UNK_IDX for token in tokens] #tokenizes 10k words\n",
    "        indices_data.append(index_list) #list of lists: indices of tokens for each sentence\n",
    "    return indices_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embedding(fname, max_count=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    counter=0\n",
    "    for line in fin:\n",
    "        counter+=1\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "        if counter==max_count:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_loc, sep=\"\\t\"):\n",
    "    #Read in data subsets\n",
    "    data = pd.read_csv(file_loc, sep=sep, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "def tokenize(data):\n",
    "    data['input1'] = data.sentence1.str.split()\n",
    "\n",
    "    data['input2'] = data.sentence2.str.split()\n",
    "    return data\n",
    "\n",
    "def assign_target(name):\n",
    "    if name == 'contradiction':\n",
    "        return 0\n",
    "    elif name == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, word2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1, self.data_list2, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.target_list) == len(self.data_list2))\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        word_idx1 = [self.word2id[c] if c in self.word2id.keys() \n",
    "                    else UNK_IDX  for c in self.data_list1[key][:MAX_WORD_LENGTH]]\n",
    "                                                                   \n",
    "        word_idx2 = [self.word2id[c] if c in self.word2id.keys() \n",
    "                    else UNK_IDX  for c in self.data_list2[key][:MAX_WORD_LENGTH]]                                                                   \n",
    "                                                                   \n",
    "        label = self.target_list[key]\n",
    "        return [word_idx1, word_idx2, len(word_idx1), len(word_idx2), label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        x1 = datum[0]\n",
    "        x2 = datum[1]\n",
    "        len1 = datum[2]\n",
    "        len2 = datum[3]\n",
    "        label = datum[4]\n",
    "        \n",
    "        label_list.append(label)\n",
    "        length_list1.append(len1)\n",
    "        length_list2.append(len2)\n",
    "        #Pad first sentences\n",
    "        padded_vec1 = np.pad(np.array(x1),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-len1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "        #Pad second sentences\n",
    "        padded_vec2 = np.pad(np.array(x2),\n",
    "                        pad_width=((0,MAX_WORD_LENGTH-len2)),\n",
    "                        mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    data_list1 = np.array(data_list1)\n",
    "    data_list2 = np.array(data_list2)\n",
    "    length_list1 = np.array(length_list1)\n",
    "    lenth_list2 = np.array(length_list2)\n",
    "    label_list = np.array(label_list)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), \n",
    "            torch.from_numpy(np.array(data_list2)),\n",
    "            torch.LongTensor(length_list1), \n",
    "            torch.LongTensor(length_list2),\n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data subsets\n",
    "train_data = read_data('../hw2_data.nosync/snli_train.tsv', sep='\\t')[0:1000]\n",
    "val_data = read_data('../hw2_data.nosync/snli_val.tsv', sep='\\t')[0:100]\n",
    "\n",
    "#Tokenize\n",
    "train_data = tokenize(train_data)\n",
    "val_data = tokenize(val_data)\n",
    "\n",
    "\n",
    "#Assign label\n",
    "train_data['target'] = train_data.label.apply(lambda x: assign_target(x))\n",
    "val_data['target'] = val_data.label.apply(lambda x: assign_target(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in pretrained embedding vectors - subset for now\n",
    "embeddings_map = load_embedding('../hw2_data.nosync/wiki-news-300d-1M.vec', max_count=50000)\n",
    "\n",
    "#Convert embedding values to lists\n",
    "embeddings = {}\n",
    "\n",
    "for key, value in embeddings_map.items():\n",
    "    embeddings[key] = list(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabulary on train set\n",
    "token2id, id2token, max_len = build_vocab(train_data['input1'] + train_data['input2'],\n",
    "                              embeddings)\n",
    "\n",
    "all_tokens = [item for sublist in train_data['input1'] + train_data['input2'] for item in sublist]\n",
    "max_len = max([len(word) for word in all_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "#Embed each input and create loaders\n",
    "\n",
    "MAX_WORD_LENGTH = max_len\n",
    "\n",
    "train_dataset = VocabDataset(zip(train_data.input1,train_data.input2, \n",
    "                                           train_data.target), token2id)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(zip(val_data.input1[:1000],val_data.input2, \n",
    "                                           val_data.target), token2id)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "#test_dataset = VocabDataset(test_data, char2id)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    " #                                          batch_size=BATCH_SIZE,\n",
    " #                                          collate_fn=vocab_collate_func,\n",
    " #                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert embedding to tensor\n",
    "import numpy as np\n",
    "\n",
    "y=np.array([np.array(list(xi)) for xi in embeddings.values()])\n",
    "padding = np.zeros((1, y.shape[1]))\n",
    "unknown = np.random.rand(1, y.shape[1]) # to account for Padding and Unknown\n",
    "full_size = np.concatenate([padding, unknown, y], axis=0)\n",
    "emb_weights = torch.from_numpy(full_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50002, 300])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement bidirectional GRU Recurrent Neural Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_weights, emb_size, hidden_size, num_layers, num_classes, vocab_size, dropout=0.5):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding =  nn.Embedding(vocab_size, emb_size, \n",
    "                                       padding_idx=PAD_IDX).from_pretrained(emb_weights, \n",
    "                                                                freeze=True) #load preset\n",
    "\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True,\n",
    "                         bidirectional=True, dropout=dropout) #creates bidirectional GRU\n",
    "        self.linear1 = nn.Linear(hidden_size*2*2, hidden_size*2*2) #2 for bidirectional, 2 for concatenated\n",
    "        self.linear2 = nn.Linear(hidden_size*2*2, num_classes) #2 for bidirectional, 2 for concatenated\n",
    "\n",
    "    \n",
    "    def forward(self, x1, x2, len1, len2):\n",
    "        \n",
    "        batch_size, seq_len = x1.size()        \n",
    "        \n",
    "        # get embedding of characters - make sure pretrained weights do not get updated\n",
    "        embed1 = self.embedding(x1)\n",
    "        embed2 = self.embedding(x2)\n",
    "        \n",
    "      \n",
    "        # fprop though RNN\n",
    "        rnn_out1, h1 = self.gru(embed1.float())\n",
    "        rnn_out2, h2 = self.gru(embed2.float())\n",
    "        \n",
    "        # [num_dir, batch_size, dim] => [batch_size, dim x num_dir]\n",
    "        num_dir, batch_size, dim = h1.shape\n",
    "        h1 = h1.transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        h2 = h2.transpose(0, 1).contiguous().view(batch_size, -1)\n",
    " \n",
    "        #Concatenate two vectors\n",
    "        combined_vector = torch.cat([h1, h2], dim=1)\n",
    "        \n",
    "        logits1 = self.linear1(combined_vector) #FC layer\n",
    "        logits2 = self.linear2(logits1) #second FC layer\n",
    "        \n",
    "        return logits2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for sentence1, sentence2, lengths1, lengths2, labels in loader:\n",
    "\n",
    "        outputs = F.softmax(model(sentence1, sentence2, lengths1, lengths2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(loader, model, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    loss_hist = []\n",
    "    for sentence1, sentence2, lengths1, lengths2, labels in loader:\n",
    "        y_hat = model(sentence1, sentence2, lengths1, lengths2)\n",
    "        loss = criterion(y_hat, labels)\n",
    "        loss_hist.append(loss.item())\n",
    "    average_loss = np.mean(loss_hist)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_plots(train_acc_hist, val_acc_hist, train_loss_epoch, train_loss_hist, val_loss_hist):\n",
    "    \n",
    "    #Accuracy\n",
    "    plt.plot(train_acc_hist, label='Train Set Accuracy')\n",
    "    plt.plot(val_acc_hist, label='Val Set Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.title('Training Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #Loss\n",
    "    plt.plot(train_loss_hist)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Iteration')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(val_loss_hist)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss per Iteration')\n",
    "    plt.show()\n",
    "    \n",
    "    print (\"Val Acc Last Epoch {}\".format(val_acc_hist[-1]))\n",
    "    print( \"Max Val Acc {}\".format(max(val_acc_hist)))\n",
    "    print( \"Avg Val Acc {}\".format(np.average(val_acc_hist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, val_loader, model, num_epochs, learning_rate):\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sentence1, sentence2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sentence1, sentence2, lengths1, lengths2)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 1 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Loss: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss.item()))\n",
    "                \n",
    "                train_acc = test_model(train_loader, model)\n",
    "                train_accs.append(train_acc)\n",
    "                val_accs.append(val_acc)\n",
    "                \n",
    "                train_losses.append(calculate_loss(train_loader, model, criterion))\n",
    "                val_losses.append(calculate_loss(val_loader, model, criterion))  \n",
    "        \n",
    "             \n",
    "    return train_losses, train_accs, val_losses, val_accs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ksenia/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1], Step: [2/32], Validation Acc: 40.0, Training Loss: 1.081879734992981\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 40.0, Training Loss: 1.1594635248184204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-64467344e35f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n\u001b[1;32m      7\u001b[0m                                                                       \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                                       num_epochs=1, learning_rate = 3e-4)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-323f6ae55933>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Loss: {}'.format(\n\u001b[1;32m     33\u001b[0m                            epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss.item()))\n",
      "\u001b[0;32m<ipython-input-72-88eecb670ca2>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-ae58c708d524>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, len1, len2)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# fprop though RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrnn_out1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mrnn_out2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mresetgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_r\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0minputgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mnewgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresetgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewgate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputgate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnewgate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Single layer bidirectional GRU\n",
    "gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=1, num_classes=3, \n",
    "            vocab_size=len(id2token), dropout=0.5)\n",
    "\n",
    "gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                      val_loader, gru_model, \n",
    "                                                                      num_epochs=1, learning_rate = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1], Step: [2/32], Validation Acc: 40.0, Training Loss: 1.0764132738113403\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 40.0, Training Loss: 1.0964765548706055\n",
      "Epoch: [1/1], Step: [4/32], Validation Acc: 40.0, Training Loss: 1.1579071283340454\n",
      "Epoch: [1/1], Step: [5/32], Validation Acc: 40.0, Training Loss: 1.144288182258606\n",
      "Epoch: [1/1], Step: [6/32], Validation Acc: 40.0, Training Loss: 1.0825755596160889\n",
      "Epoch: [1/1], Step: [7/32], Validation Acc: 36.0, Training Loss: 1.0659681558609009\n",
      "Epoch: [1/1], Step: [8/32], Validation Acc: 37.0, Training Loss: 1.0935747623443604\n",
      "Epoch: [1/1], Step: [9/32], Validation Acc: 33.0, Training Loss: 1.1189136505126953\n",
      "Epoch: [1/1], Step: [10/32], Validation Acc: 32.0, Training Loss: 1.1239734888076782\n",
      "Epoch: [1/1], Step: [11/32], Validation Acc: 31.0, Training Loss: 1.1283109188079834\n",
      "Epoch: [1/1], Step: [12/32], Validation Acc: 30.0, Training Loss: 1.094236969947815\n",
      "Epoch: [1/1], Step: [13/32], Validation Acc: 30.0, Training Loss: 1.0931586027145386\n",
      "Epoch: [1/1], Step: [14/32], Validation Acc: 34.0, Training Loss: 1.1030352115631104\n",
      "Epoch: [1/1], Step: [15/32], Validation Acc: 42.0, Training Loss: 1.0842843055725098\n",
      "Epoch: [1/1], Step: [16/32], Validation Acc: 34.0, Training Loss: 1.0952585935592651\n",
      "Epoch: [1/1], Step: [17/32], Validation Acc: 34.0, Training Loss: 1.071683645248413\n",
      "Epoch: [1/1], Step: [18/32], Validation Acc: 34.0, Training Loss: 1.1173546314239502\n",
      "Epoch: [1/1], Step: [19/32], Validation Acc: 34.0, Training Loss: 1.0922863483428955\n",
      "Epoch: [1/1], Step: [20/32], Validation Acc: 34.0, Training Loss: 1.109231948852539\n",
      "Epoch: [1/1], Step: [21/32], Validation Acc: 34.0, Training Loss: 1.145942211151123\n",
      "Epoch: [1/1], Step: [22/32], Validation Acc: 34.0, Training Loss: 1.05106520652771\n",
      "Epoch: [1/1], Step: [23/32], Validation Acc: 34.0, Training Loss: 1.130397081375122\n",
      "Epoch: [1/1], Step: [24/32], Validation Acc: 33.0, Training Loss: 1.05655038356781\n",
      "Epoch: [1/1], Step: [25/32], Validation Acc: 34.0, Training Loss: 1.1275979280471802\n",
      "Epoch: [1/1], Step: [26/32], Validation Acc: 35.0, Training Loss: 1.0485515594482422\n",
      "Epoch: [1/1], Step: [27/32], Validation Acc: 35.0, Training Loss: 1.1095657348632812\n",
      "Epoch: [1/1], Step: [28/32], Validation Acc: 40.0, Training Loss: 1.1209591627120972\n",
      "Epoch: [1/1], Step: [29/32], Validation Acc: 40.0, Training Loss: 1.1174430847167969\n",
      "Epoch: [1/1], Step: [30/32], Validation Acc: 39.0, Training Loss: 1.103023648262024\n",
      "Epoch: [1/1], Step: [31/32], Validation Acc: 40.0, Training Loss: 1.0835195779800415\n",
      "Epoch: [1/1], Step: [32/32], Validation Acc: 45.0, Training Loss: 1.0904631614685059\n"
     ]
    }
   ],
   "source": [
    "gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=1, num_classes=3, \n",
    "            vocab_size=len(id2token), dropout=0)\n",
    "\n",
    "gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                      val_loader, gru_model, \n",
    "                                                                      num_epochs=1, learning_rate = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gru_train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-c9fff7e60ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_train_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_train_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_val_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_val_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gru_train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "make_plots(gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Size dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1], Step: [2/32], Validation Acc: 34.0, Training Loss: 1.0942107439041138\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 34.0, Training Loss: 1.0986782312393188\n",
      "Epoch: [1/1], Step: [4/32], Validation Acc: 36.0, Training Loss: 1.0928051471710205\n",
      "Epoch: [1/1], Step: [5/32], Validation Acc: 37.0, Training Loss: 1.1051435470581055\n",
      "Epoch: [1/1], Step: [6/32], Validation Acc: 41.0, Training Loss: 1.0966055393218994\n",
      "Epoch: [1/1], Step: [7/32], Validation Acc: 37.0, Training Loss: 1.0905410051345825\n",
      "Epoch: [1/1], Step: [8/32], Validation Acc: 38.0, Training Loss: 1.080612063407898\n",
      "Epoch: [1/1], Step: [9/32], Validation Acc: 38.0, Training Loss: 1.1300971508026123\n",
      "Epoch: [1/1], Step: [10/32], Validation Acc: 35.0, Training Loss: 1.1146891117095947\n",
      "Epoch: [1/1], Step: [11/32], Validation Acc: 35.0, Training Loss: 1.1196895837783813\n",
      "Epoch: [1/1], Step: [12/32], Validation Acc: 35.0, Training Loss: 1.1068370342254639\n",
      "Epoch: [1/1], Step: [13/32], Validation Acc: 38.0, Training Loss: 1.1191234588623047\n",
      "Epoch: [1/1], Step: [14/32], Validation Acc: 42.0, Training Loss: 1.076533555984497\n",
      "Epoch: [1/1], Step: [15/32], Validation Acc: 39.0, Training Loss: 1.1124420166015625\n",
      "Epoch: [1/1], Step: [16/32], Validation Acc: 39.0, Training Loss: 1.0986310243606567\n",
      "Epoch: [1/1], Step: [17/32], Validation Acc: 36.0, Training Loss: 1.101388931274414\n",
      "Epoch: [1/1], Step: [18/32], Validation Acc: 36.0, Training Loss: 1.0959546566009521\n",
      "Epoch: [1/1], Step: [19/32], Validation Acc: 33.0, Training Loss: 1.0913504362106323\n",
      "Epoch: [1/1], Step: [20/32], Validation Acc: 34.0, Training Loss: 1.0929044485092163\n",
      "Epoch: [1/1], Step: [21/32], Validation Acc: 33.0, Training Loss: 1.0998340845108032\n",
      "Epoch: [1/1], Step: [22/32], Validation Acc: 32.0, Training Loss: 1.0944255590438843\n",
      "Epoch: [1/1], Step: [23/32], Validation Acc: 34.0, Training Loss: 1.1014161109924316\n",
      "Epoch: [1/1], Step: [24/32], Validation Acc: 33.0, Training Loss: 1.099517822265625\n",
      "Epoch: [1/1], Step: [25/32], Validation Acc: 34.0, Training Loss: 1.1109365224838257\n",
      "Epoch: [1/1], Step: [26/32], Validation Acc: 33.0, Training Loss: 1.0940532684326172\n",
      "Epoch: [1/1], Step: [27/32], Validation Acc: 31.0, Training Loss: 1.0933386087417603\n",
      "Epoch: [1/1], Step: [28/32], Validation Acc: 33.0, Training Loss: 1.1146912574768066\n",
      "Epoch: [1/1], Step: [29/32], Validation Acc: 33.0, Training Loss: 1.0923399925231934\n",
      "Epoch: [1/1], Step: [30/32], Validation Acc: 37.0, Training Loss: 1.102413296699524\n",
      "Epoch: [1/1], Step: [31/32], Validation Acc: 32.0, Training Loss: 1.1049180030822754\n",
      "Epoch: [1/1], Step: [32/32], Validation Acc: 34.0, Training Loss: 1.10355544090271\n",
      "Epoch: [1/1], Step: [2/32], Validation Acc: 26.0, Training Loss: 1.0974376201629639\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 26.0, Training Loss: 1.095631718635559\n",
      "Epoch: [1/1], Step: [4/32], Validation Acc: 26.0, Training Loss: 1.1127269268035889\n",
      "Epoch: [1/1], Step: [5/32], Validation Acc: 26.0, Training Loss: 1.1648889780044556\n",
      "Epoch: [1/1], Step: [6/32], Validation Acc: 26.0, Training Loss: 1.1105902194976807\n",
      "Epoch: [1/1], Step: [7/32], Validation Acc: 26.0, Training Loss: 1.0895863771438599\n",
      "Epoch: [1/1], Step: [8/32], Validation Acc: 26.0, Training Loss: 1.0982908010482788\n",
      "Epoch: [1/1], Step: [9/32], Validation Acc: 30.0, Training Loss: 1.11893630027771\n",
      "Epoch: [1/1], Step: [10/32], Validation Acc: 34.0, Training Loss: 1.102052927017212\n",
      "Epoch: [1/1], Step: [11/32], Validation Acc: 35.0, Training Loss: 1.099510669708252\n",
      "Epoch: [1/1], Step: [12/32], Validation Acc: 37.0, Training Loss: 1.100300669670105\n",
      "Epoch: [1/1], Step: [13/32], Validation Acc: 34.0, Training Loss: 1.0971617698669434\n",
      "Epoch: [1/1], Step: [14/32], Validation Acc: 35.0, Training Loss: 1.0933841466903687\n",
      "Epoch: [1/1], Step: [15/32], Validation Acc: 35.0, Training Loss: 1.1066555976867676\n",
      "Epoch: [1/1], Step: [16/32], Validation Acc: 34.0, Training Loss: 1.1032460927963257\n",
      "Epoch: [1/1], Step: [17/32], Validation Acc: 34.0, Training Loss: 1.1138979196548462\n",
      "Epoch: [1/1], Step: [18/32], Validation Acc: 34.0, Training Loss: 1.1018831729888916\n",
      "Epoch: [1/1], Step: [19/32], Validation Acc: 34.0, Training Loss: 1.0695611238479614\n",
      "Epoch: [1/1], Step: [20/32], Validation Acc: 34.0, Training Loss: 1.1062270402908325\n",
      "Epoch: [1/1], Step: [21/32], Validation Acc: 33.0, Training Loss: 1.096805214881897\n",
      "Epoch: [1/1], Step: [22/32], Validation Acc: 32.0, Training Loss: 1.0933910608291626\n",
      "Epoch: [1/1], Step: [23/32], Validation Acc: 32.0, Training Loss: 1.0728960037231445\n",
      "Epoch: [1/1], Step: [24/32], Validation Acc: 32.0, Training Loss: 1.0845451354980469\n",
      "Epoch: [1/1], Step: [25/32], Validation Acc: 33.0, Training Loss: 1.110039472579956\n",
      "Epoch: [1/1], Step: [26/32], Validation Acc: 34.0, Training Loss: 1.1192433834075928\n",
      "Epoch: [1/1], Step: [27/32], Validation Acc: 33.0, Training Loss: 1.130340337753296\n",
      "Epoch: [1/1], Step: [28/32], Validation Acc: 33.0, Training Loss: 1.085487961769104\n",
      "Epoch: [1/1], Step: [29/32], Validation Acc: 34.0, Training Loss: 1.074249029159546\n",
      "Epoch: [1/1], Step: [30/32], Validation Acc: 33.0, Training Loss: 1.1020528078079224\n",
      "Epoch: [1/1], Step: [31/32], Validation Acc: 39.0, Training Loss: 1.1029030084609985\n",
      "Epoch: [1/1], Step: [32/32], Validation Acc: 41.0, Training Loss: 1.1338393688201904\n",
      "Epoch: [1/1], Step: [2/32], Validation Acc: 34.0, Training Loss: 1.095697045326233\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 40.0, Training Loss: 1.1381864547729492\n",
      "Epoch: [1/1], Step: [4/32], Validation Acc: 40.0, Training Loss: 1.0942041873931885\n",
      "Epoch: [1/1], Step: [5/32], Validation Acc: 40.0, Training Loss: 1.1061367988586426\n",
      "Epoch: [1/1], Step: [6/32], Validation Acc: 40.0, Training Loss: 1.1548917293548584\n",
      "Epoch: [1/1], Step: [7/32], Validation Acc: 39.0, Training Loss: 1.0844039916992188\n",
      "Epoch: [1/1], Step: [8/32], Validation Acc: 38.0, Training Loss: 1.1092844009399414\n",
      "Epoch: [1/1], Step: [9/32], Validation Acc: 38.0, Training Loss: 1.0790163278579712\n",
      "Epoch: [1/1], Step: [10/32], Validation Acc: 40.0, Training Loss: 1.0846012830734253\n",
      "Epoch: [1/1], Step: [11/32], Validation Acc: 39.0, Training Loss: 1.0919510126113892\n",
      "Epoch: [1/1], Step: [12/32], Validation Acc: 41.0, Training Loss: 1.0894231796264648\n",
      "Epoch: [1/1], Step: [13/32], Validation Acc: 40.0, Training Loss: 1.0880450010299683\n",
      "Epoch: [1/1], Step: [14/32], Validation Acc: 39.0, Training Loss: 1.076387643814087\n",
      "Epoch: [1/1], Step: [15/32], Validation Acc: 40.0, Training Loss: 1.118218183517456\n",
      "Epoch: [1/1], Step: [16/32], Validation Acc: 45.0, Training Loss: 1.1079840660095215\n",
      "Epoch: [1/1], Step: [17/32], Validation Acc: 49.0, Training Loss: 1.1020208597183228\n",
      "Epoch: [1/1], Step: [18/32], Validation Acc: 40.0, Training Loss: 1.106088638305664\n",
      "Epoch: [1/1], Step: [19/32], Validation Acc: 37.0, Training Loss: 1.103177547454834\n",
      "Epoch: [1/1], Step: [20/32], Validation Acc: 36.0, Training Loss: 1.1087758541107178\n",
      "Epoch: [1/1], Step: [21/32], Validation Acc: 36.0, Training Loss: 1.0866283178329468\n",
      "Epoch: [1/1], Step: [22/32], Validation Acc: 36.0, Training Loss: 1.1116939783096313\n",
      "Epoch: [1/1], Step: [23/32], Validation Acc: 37.0, Training Loss: 1.0985609292984009\n",
      "Epoch: [1/1], Step: [24/32], Validation Acc: 37.0, Training Loss: 1.0919711589813232\n",
      "Epoch: [1/1], Step: [25/32], Validation Acc: 36.0, Training Loss: 1.0720661878585815\n",
      "Epoch: [1/1], Step: [26/32], Validation Acc: 35.0, Training Loss: 1.0658420324325562\n",
      "Epoch: [1/1], Step: [27/32], Validation Acc: 37.0, Training Loss: 1.1175768375396729\n",
      "Epoch: [1/1], Step: [28/32], Validation Acc: 37.0, Training Loss: 1.0594062805175781\n",
      "Epoch: [1/1], Step: [29/32], Validation Acc: 37.0, Training Loss: 1.101163625717163\n",
      "Epoch: [1/1], Step: [30/32], Validation Acc: 35.0, Training Loss: 1.0770446062088013\n",
      "Epoch: [1/1], Step: [31/32], Validation Acc: 34.0, Training Loss: 1.0883355140686035\n",
      "Epoch: [1/1], Step: [32/32], Validation Acc: 33.0, Training Loss: 1.0728998184204102\n",
      "Epoch: [1/1], Step: [2/32], Validation Acc: 33.0, Training Loss: 1.1182606220245361\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 27.0, Training Loss: 1.11381995677948\n",
      "Epoch: [1/1], Step: [4/32], Validation Acc: 31.0, Training Loss: 1.0943841934204102\n",
      "Epoch: [1/1], Step: [5/32], Validation Acc: 32.0, Training Loss: 1.1038732528686523\n",
      "Epoch: [1/1], Step: [6/32], Validation Acc: 39.0, Training Loss: 1.0826786756515503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/1], Step: [7/32], Validation Acc: 42.0, Training Loss: 1.11130952835083\n",
      "Epoch: [1/1], Step: [8/32], Validation Acc: 33.0, Training Loss: 1.0682414770126343\n",
      "Epoch: [1/1], Step: [9/32], Validation Acc: 30.0, Training Loss: 1.0564875602722168\n",
      "Epoch: [1/1], Step: [10/32], Validation Acc: 30.0, Training Loss: 1.0630210638046265\n",
      "Epoch: [1/1], Step: [11/32], Validation Acc: 30.0, Training Loss: 1.1325324773788452\n",
      "Epoch: [1/1], Step: [12/32], Validation Acc: 31.0, Training Loss: 1.090908408164978\n",
      "Epoch: [1/1], Step: [13/32], Validation Acc: 30.0, Training Loss: 1.173345685005188\n",
      "Epoch: [1/1], Step: [14/32], Validation Acc: 29.0, Training Loss: 1.1883749961853027\n",
      "Epoch: [1/1], Step: [15/32], Validation Acc: 33.0, Training Loss: 1.1041111946105957\n",
      "Epoch: [1/1], Step: [16/32], Validation Acc: 39.0, Training Loss: 1.1010730266571045\n",
      "Epoch: [1/1], Step: [17/32], Validation Acc: 42.0, Training Loss: 1.0474096536636353\n",
      "Epoch: [1/1], Step: [18/32], Validation Acc: 40.0, Training Loss: 1.065883755683899\n",
      "Epoch: [1/1], Step: [19/32], Validation Acc: 35.0, Training Loss: 1.0666582584381104\n",
      "Epoch: [1/1], Step: [20/32], Validation Acc: 33.0, Training Loss: 1.0802291631698608\n",
      "Epoch: [1/1], Step: [21/32], Validation Acc: 35.0, Training Loss: 1.1496968269348145\n",
      "Epoch: [1/1], Step: [22/32], Validation Acc: 36.0, Training Loss: 1.1016789674758911\n",
      "Epoch: [1/1], Step: [23/32], Validation Acc: 36.0, Training Loss: 1.1594029664993286\n",
      "Epoch: [1/1], Step: [24/32], Validation Acc: 38.0, Training Loss: 1.101020097732544\n",
      "Epoch: [1/1], Step: [25/32], Validation Acc: 38.0, Training Loss: 1.076337456703186\n",
      "Epoch: [1/1], Step: [26/32], Validation Acc: 37.0, Training Loss: 1.0399171113967896\n",
      "Epoch: [1/1], Step: [27/32], Validation Acc: 36.0, Training Loss: 1.1281251907348633\n",
      "Epoch: [1/1], Step: [28/32], Validation Acc: 34.0, Training Loss: 1.1206828355789185\n",
      "Epoch: [1/1], Step: [29/32], Validation Acc: 32.0, Training Loss: 1.0891472101211548\n",
      "Epoch: [1/1], Step: [30/32], Validation Acc: 33.0, Training Loss: 1.1034127473831177\n",
      "Epoch: [1/1], Step: [31/32], Validation Acc: 33.0, Training Loss: 1.1198447942733765\n",
      "Epoch: [1/1], Step: [32/32], Validation Acc: 36.0, Training Loss: 1.1168917417526245\n",
      "Epoch: [1/1], Step: [2/32], Validation Acc: 34.0, Training Loss: 1.0712637901306152\n",
      "Epoch: [1/1], Step: [3/32], Validation Acc: 34.0, Training Loss: 1.2012099027633667\n",
      "Epoch: [1/1], Step: [4/32], Validation Acc: 32.0, Training Loss: 1.1063787937164307\n",
      "Epoch: [1/1], Step: [5/32], Validation Acc: 33.0, Training Loss: 1.0493441820144653\n",
      "Epoch: [1/1], Step: [6/32], Validation Acc: 33.0, Training Loss: 1.0977966785430908\n",
      "Epoch: [1/1], Step: [7/32], Validation Acc: 26.0, Training Loss: 1.1546012163162231\n",
      "Epoch: [1/1], Step: [8/32], Validation Acc: 26.0, Training Loss: 1.168158769607544\n",
      "Epoch: [1/1], Step: [9/32], Validation Acc: 27.0, Training Loss: 1.1198410987854004\n",
      "Epoch: [1/1], Step: [10/32], Validation Acc: 24.0, Training Loss: 1.089798927307129\n",
      "Epoch: [1/1], Step: [11/32], Validation Acc: 27.0, Training Loss: 1.068325400352478\n",
      "Epoch: [1/1], Step: [12/32], Validation Acc: 38.0, Training Loss: 1.1619926691055298\n",
      "Epoch: [1/1], Step: [13/32], Validation Acc: 40.0, Training Loss: 1.1197290420532227\n",
      "Epoch: [1/1], Step: [14/32], Validation Acc: 39.0, Training Loss: 1.1138882637023926\n",
      "Epoch: [1/1], Step: [15/32], Validation Acc: 41.0, Training Loss: 1.1408097743988037\n",
      "Epoch: [1/1], Step: [16/32], Validation Acc: 42.0, Training Loss: 1.1102280616760254\n",
      "Epoch: [1/1], Step: [17/32], Validation Acc: 39.0, Training Loss: 1.0905284881591797\n",
      "Epoch: [1/1], Step: [18/32], Validation Acc: 42.0, Training Loss: 1.0588243007659912\n",
      "Epoch: [1/1], Step: [19/32], Validation Acc: 42.0, Training Loss: 1.1230180263519287\n",
      "Epoch: [1/1], Step: [20/32], Validation Acc: 37.0, Training Loss: 1.124367117881775\n",
      "Epoch: [1/1], Step: [21/32], Validation Acc: 39.0, Training Loss: 1.1174226999282837\n",
      "Epoch: [1/1], Step: [22/32], Validation Acc: 38.0, Training Loss: 1.0876986980438232\n",
      "Epoch: [1/1], Step: [23/32], Validation Acc: 40.0, Training Loss: 1.108119010925293\n",
      "Epoch: [1/1], Step: [24/32], Validation Acc: 39.0, Training Loss: 1.0950777530670166\n",
      "Epoch: [1/1], Step: [25/32], Validation Acc: 39.0, Training Loss: 1.1078152656555176\n",
      "Epoch: [1/1], Step: [26/32], Validation Acc: 36.0, Training Loss: 1.0952281951904297\n",
      "Epoch: [1/1], Step: [27/32], Validation Acc: 35.0, Training Loss: 1.1011558771133423\n",
      "Epoch: [1/1], Step: [28/32], Validation Acc: 32.0, Training Loss: 1.0894156694412231\n",
      "Epoch: [1/1], Step: [29/32], Validation Acc: 32.0, Training Loss: 1.0967724323272705\n",
      "Epoch: [1/1], Step: [30/32], Validation Acc: 32.0, Training Loss: 1.1547600030899048\n",
      "Epoch: [1/1], Step: [31/32], Validation Acc: 31.0, Training Loss: 1.0609378814697266\n",
      "Epoch: [1/1], Step: [32/32], Validation Acc: 34.0, Training Loss: 0.9411640167236328\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [50, 100, 200, 300, 500]\n",
    "\n",
    "hidden_results = {}\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    \n",
    "    gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "                hidden_size=h, num_layers=1, num_classes=3, \n",
    "                vocab_size=len(id2token))\n",
    "\n",
    "    gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, gru_model, \n",
    "                                                                          num_epochs=1, learning_rate = 3e-4)\n",
    "    \n",
    "    hidden_results[h] = [gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.0,\n",
       " 34.0,\n",
       " 36.0,\n",
       " 37.0,\n",
       " 41.0,\n",
       " 37.0,\n",
       " 38.0,\n",
       " 38.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 35.0,\n",
       " 38.0,\n",
       " 42.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 33.0,\n",
       " 32.0,\n",
       " 34.0,\n",
       " 33.0,\n",
       " 34.0,\n",
       " 33.0,\n",
       " 31.0,\n",
       " 33.0,\n",
       " 33.0,\n",
       " 37.0,\n",
       " 32.0,\n",
       " 34.0]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_results[50][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets implement basic Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_weights, emb_size, hidden_size, num_layers, num_classes, vocab_size):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding =  nn.Embedding(vocab_size, emb_size, \n",
    "                                       padding_idx=PAD_IDX).from_pretrained(emb_weights, \n",
    "                                                                freeze=True) #load preset\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size) #2 for concatenated\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes) #2 for bidirectional, 2 for concatenated\n",
    "    \n",
    "     \n",
    "\n",
    "    def forward(self, x1, x2, len1, len2):\n",
    "        \n",
    "        batch_size, seq_len = x1.size()\n",
    "            \n",
    "        embed1 = self.embedding(x1)\n",
    "        hidden = self.conv1(embed1.float().transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        #max pool over time\n",
    "        hidden1 = torch.max(hidden, dim=1)[0]\n",
    "        \n",
    "        #Second sentence pass\n",
    "        \n",
    "        embed2 = self.embedding(x2)\n",
    "        hidden2 = self.conv1(embed2.float().transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        #max pool over time\n",
    "        hidden2 = torch.max(hidden2, dim=1)[0]\n",
    "        \n",
    "        #Concatenate two vectors\n",
    "        combined_vector = torch.cat([hidden1, hidden2], dim=1)\n",
    "        \n",
    "        logits1 = self.linear1(combined_vector)\n",
    "        logits2 = self.linear2(logits1)\n",
    "        \n",
    "        return logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/50], Step: [101/3125], Validation Acc: 54.9, Training Loss: 0.9760991930961609\n",
      "Epoch: [1/50], Step: [201/3125], Validation Acc: 57.5, Training Loss: 0.9982364177703857\n",
      "Epoch: [1/50], Step: [301/3125], Validation Acc: 57.1, Training Loss: 0.9656170606613159\n",
      "Epoch: [1/50], Step: [401/3125], Validation Acc: 55.5, Training Loss: 0.9312114119529724\n",
      "Epoch: [1/50], Step: [501/3125], Validation Acc: 56.5, Training Loss: 0.8374489545822144\n",
      "Epoch: [1/50], Step: [601/3125], Validation Acc: 56.8, Training Loss: 0.8368998765945435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-9ac9b37edf0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m             vocab_size=len(id2token))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcnn_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-5d5801788657>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m#print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-7287e835fcf3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, len1, len2)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# fprop though RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mrnn_out1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrnn_out2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# [num_dir, batch_size, dim] => [batch_size, dim x num_dir]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight, batch_sizes)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;31m# hack to handle LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mGRUCell\u001b[0;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mgi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mgh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mi_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mh_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlp/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Single layer bidirectional GRU\n",
    "cnn_model = CNN(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=2, num_classes=3, \n",
    "            vocab_size=len(id2token))\n",
    "\n",
    "cnn_losses, cnn_accuracies = train(train_loader, val_loader, gru_model, num_epochs=50, learning_rate = 3e-4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
