{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code was run on Google Colab website using GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UfeMMvaxe689",
    "outputId": "6a33dec8-d222-4dcf-ccb1-7b2c15f169f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073750016 bytes == 0x591d6000 @  0x7f1de77e62a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n"
     ]
    }
   ],
   "source": [
    "# First, we will load packages necessary to run pytorch\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qlPrfsvmd__q"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "random.seed(134)\n",
    "import pandas as pd\n",
    "import io\n",
    "from google.colab import drive\n",
    "import json\n",
    "\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "qOWn9tiOi0Bo",
    "outputId": "8c12c343-fc4a-4f02-ead0-c504d5e2fb8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#Mount google drive\n",
    "drive.mount('/content/gdrive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AluY1FPgjRwd",
    "outputId": "ce897758-9ed7-49e4-b4ec-f1d649f4277a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawl-300d-2M.vec\t       mnli_val_subset.csv  snli_val.tsv\n",
      "data_understanding_prep.ipynb  mnli_val.tsv\t    wiki-news-300d-1M.vec\n",
      "mnli_train_subset.csv\t       snli_train1.tsv\n",
      "mnli_train.tsv\t\t       snli_train.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls 'gdrive/My Drive/mnli/hw2_data.nosync'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZ6SLi6Xd__z"
   },
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DW9zUy_dd__1"
   },
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens, embedding, max_vocab_size = 10000):\n",
    "\n",
    "    # save index 1 for unk and 0 for pad\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    all_tokens = [item for sublist in all_tokens for item in sublist]\n",
    "    max_len = max([len(word) for word in all_tokens])\n",
    "    \n",
    "    unique_words = list(embedding.keys())\n",
    "    \n",
    "    id2token =  unique_words #list of words available in embedding\n",
    "    id2token = ['<pad>', '<unk>'] + id2token #add pad and unknown to the beginning\n",
    "    \n",
    "    token2id = dict(zip(unique_words, range(2,2+len(unique_words)))) # dictionary of words and indices \n",
    "    token2id['<pad>'] = PAD_IDX  #add pad symbol to the dictionary\n",
    "    token2id['<unk>'] = UNK_IDX  #add unkown symbol to the dictionary\n",
    "    \n",
    "    return token2id, id2token, max_len\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    PAD_IDX = 0\n",
    "    UNK_IDX = 1\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [list(token2id[token]) if token in token2id else UNK_IDX for token in tokens] #tokenizes 10k words\n",
    "        indices_data.append(index_list) #list of lists: indices of tokens for each sentence\n",
    "    return indices_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F_b1VXsjd__6"
   },
   "outputs": [],
   "source": [
    "def load_embedding(fname, max_count=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    counter=0\n",
    "    for line in fin:\n",
    "        counter+=1\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "        if counter==max_count:\n",
    "            break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lbcIaraCd___"
   },
   "outputs": [],
   "source": [
    "def read_data(file_loc, sep=\"\\t\"):\n",
    "    #Read in data subsets\n",
    "    data = pd.read_csv(file_loc, sep=sep, encoding='latin-1')\n",
    "    return data\n",
    "\n",
    "def tokenize(data):\n",
    "    data['input1'] = data.sentence1.str.split()\n",
    "\n",
    "    data['input2'] = data.sentence2.str.split()\n",
    "    return data\n",
    "\n",
    "def assign_target(name):\n",
    "    if name == 'contradiction':\n",
    "        return 0\n",
    "    elif name == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LKCZ6SzeeAAE"
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tuple, word2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        @param target_list: list of targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list1, self.data_list2, self.target_list = zip(*data_tuple)\n",
    "        assert (len(self.data_list1) == len(self.target_list) == len(self.data_list2))\n",
    "        self.word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        word_idx1 = [self.word2id[c] if c in self.word2id.keys() \n",
    "                    else UNK_IDX  for c in self.data_list1[key][:MAX_WORD_LENGTH]]\n",
    "                                                                   \n",
    "        word_idx2 = [self.word2id[c] if c in self.word2id.keys() \n",
    "                    else UNK_IDX  for c in self.data_list2[key][:MAX_WORD_LENGTH]]                                                                   \n",
    "                                                                   \n",
    "        label = self.target_list[key]\n",
    "        return [word_idx1, word_idx2, len(word_idx1), len(word_idx2), label]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    label_list = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        x1 = datum[0]\n",
    "        x2 = datum[1]\n",
    "        len1 = datum[2]\n",
    "        len2 = datum[3]\n",
    "        label = datum[4]\n",
    "        \n",
    "        label_list.append(label)\n",
    "        length_list1.append(len1)\n",
    "        length_list2.append(len2)\n",
    "        #Pad first sentences\n",
    "        padded_vec1 = np.pad(np.array(x1),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-len1)),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list1.append(padded_vec1)\n",
    "        \n",
    "        #Pad second sentences\n",
    "        padded_vec2 = np.pad(np.array(x2),\n",
    "                        pad_width=((0,MAX_WORD_LENGTH-len2)),\n",
    "                        mode=\"constant\", constant_values=0)\n",
    "        data_list2.append(padded_vec2)\n",
    "        \n",
    "    data_list1 = np.array(data_list1)\n",
    "    data_list2 = np.array(data_list2)\n",
    "    length_list1 = np.array(length_list1)\n",
    "    lenth_list2 = np.array(length_list2)\n",
    "    label_list = np.array(label_list)\n",
    "    \n",
    "    return [torch.from_numpy(np.array(data_list1)), \n",
    "            torch.from_numpy(np.array(data_list2)),\n",
    "            torch.LongTensor(length_list1), \n",
    "            torch.LongTensor(length_list2),\n",
    "            torch.LongTensor(label_list)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GAssECDeAAJ"
   },
   "source": [
    "## Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "U9ND-YTIeAAU"
   },
   "outputs": [],
   "source": [
    "#Read in data subsets\n",
    "train_data = read_data('gdrive/My Drive/mnli/hw2_data.nosync/snli_train.tsv', sep='\\t')\n",
    "val_data = read_data('gdrive/My Drive/mnli/hw2_data.nosync/snli_val.tsv', sep='\\t')\n",
    "\n",
    "#Tokenize\n",
    "train_data = tokenize(train_data)\n",
    "val_data = tokenize(val_data)\n",
    "\n",
    "\n",
    "#Assign label\n",
    "train_data['target'] = train_data.label.apply(lambda x: assign_target(x))\n",
    "val_data['target'] = val_data.label.apply(lambda x: assign_target(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N0sHnq3KeAAZ"
   },
   "outputs": [],
   "source": [
    "#Read in pretrained embedding vectors - subset for now\n",
    "embeddings_map = load_embedding('gdrive/My Drive/mnli/hw2_data.nosync/wiki-news-300d-1M.vec', max_count=50000)\n",
    "\n",
    "#Convert embedding values to lists\n",
    "embeddings = {}\n",
    "\n",
    "for key, value in embeddings_map.items():\n",
    "    embeddings[key] = list(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vxEW2lOreAAf"
   },
   "outputs": [],
   "source": [
    "#Build vocabulary on train set\n",
    "token2id, id2token, max_len = build_vocab(train_data['input1'] + train_data['input2'],\n",
    "                              embeddings)\n",
    "\n",
    "all_tokens = [item for sublist in train_data['input1'] + train_data['input2'] for item in sublist]\n",
    "max_len = max([len(word) for word in all_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0aqvLy8yeAAk"
   },
   "outputs": [],
   "source": [
    "# Build train, valid and test dataloaders\n",
    "\n",
    "#Embed each input and create loaders\n",
    "\n",
    "MAX_WORD_LENGTH = max_len\n",
    "\n",
    "train_dataset = VocabDataset(zip(train_data.input1,train_data.input2, \n",
    "                                           train_data.target), token2id)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(zip(val_data.input1,val_data.input2, \n",
    "                                           val_data.target), token2id)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BRILy-aoeAAq"
   },
   "outputs": [],
   "source": [
    "#Convert embedding to tensor\n",
    "import numpy as np\n",
    "\n",
    "y=np.array([np.array(list(xi)) for xi in embeddings.values()])\n",
    "padding = np.zeros((1, y.shape[1]))\n",
    "unknown = np.random.rand(1, y.shape[1]) # to account for Padding and Unknown\n",
    "full_size = np.concatenate([padding, unknown, y], axis=0)\n",
    "emb_weights = torch.from_numpy(full_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yGQhJW8eAA9"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIbFA38UeABA"
   },
   "source": [
    "### Now lets implement bidirectional GRU Recurrent Neural Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "l887eogXeABB"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_weights, emb_size, hidden_size, num_layers, num_classes, vocab_size, dropout=0):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        # vocab_size: vocabulary size\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding =  nn.Embedding(vocab_size, emb_size, \n",
    "                                       padding_idx=PAD_IDX).from_pretrained(emb_weights, \n",
    "                                                                freeze=True).to(device) #load preset\n",
    "\n",
    "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True,\n",
    "                         bidirectional=True, dropout=dropout).to(device) #creates bidirectional GRU\n",
    "        self.linear1 = nn.Linear(hidden_size*2*2, hidden_size*2*2).to(device) #2 for bidirectional, 2 for concatenated\n",
    "        self.linear2 = nn.Linear(hidden_size*2*2, num_classes).to(device) #2 for bidirectional, 2 for concatenated\n",
    "        #self.linear1 = nn.Linear(hidden_size*2*2, num_classes).to(device)\n",
    "\n",
    "        \n",
    "    def forward(self, x1, x2, len1, len2):\n",
    "        \n",
    "        # sorts in each forward pass\n",
    "\n",
    "        batch_size, seq_len = x1.size()\n",
    " \n",
    "        # get embedding of characters - make sure pretrained weights do not get updated\n",
    "        embed1 = self.embedding(x1.to(device))\n",
    "        embed2 = self.embedding(x2.to(device))\n",
    "        \n",
    "      \n",
    "        # fprop though RNN\n",
    "        rnn_out1, h1 = self.gru(embed1.float())\n",
    "        rnn_out2, h2 = self.gru(embed2.float())\n",
    "        \n",
    "        # [num_dir, batch_size, dim] => [batch_size, dim x num_dir]\n",
    "        num_dir, batch_size, dim = h1.shape\n",
    "        h1 = h1.transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        h2 = h2.transpose(0, 1).contiguous().view(batch_size, -1)\n",
    "        \n",
    "        #Concatenate two vectors\n",
    "        combined_vector = torch.cat([h1, h2], dim=1)\n",
    "        \n",
    "        logits1 = self.linear1(combined_vector) #FC layer\n",
    "        logits2 = self.linear2(logits1) #second FC layer\n",
    "        \n",
    "        return logits2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FDpzeI97eABH"
   },
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for sentence1, sentence2, lengths1, lengths2, labels in loader:\n",
    "\n",
    "        outputs = F.softmax(model(sentence1, sentence2, lengths1, lengths2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1].to(device)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.to(device).view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "95eqAv-Hn7VT"
   },
   "outputs": [],
   "source": [
    "def calculate_loss(loader, model, criterion):\n",
    "    '''Calculate loss for evaluation'''\n",
    "    model.eval()\n",
    "    loss_hist = []\n",
    "    for x1, x2, len1, len2, labels in loader:\n",
    "      \n",
    "        y_hat = model(x1, x2, len1, len2).to(device)\n",
    "        loss = criterion(y_hat, labels.to(device))\n",
    "        loss_hist.append(loss.item())\n",
    "    average_loss = np.mean(loss_hist)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "22fdGJyqeABO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(train_loader, val_loader, model, num_epochs, learning_rate, decay=0):\n",
    "    '''Train model'''\n",
    "    \n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (sentence1, sentence2, lengths1, lengths2, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sentence1, sentence2, lengths1, lengths2)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # validate every 1000 iterations\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Training Loss: {}'.format(\n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss.item()))\n",
    "                \n",
    "                \n",
    "                train_acc = test_model(train_loader, model)\n",
    "                train_accs.append(train_acc)\n",
    "                val_accs.append(val_acc)\n",
    "\n",
    "                train_losses.append(calculate_loss(train_loader, model, criterion))\n",
    "                val_losses.append(calculate_loss(val_loader, model, criterion))\n",
    "                \n",
    "    return train_losses, train_accs, val_losses, val_accs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HiyQyAoUeABV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Single layer bidirectional GRU\n",
    "gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=1, num_classes=3, \n",
    "            vocab_size=len(id2token))\n",
    "\n",
    "gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc = train(train_loader, \n",
    "                                                                     val_loader, \n",
    "                                                                     gru_model, num_epochs=1, \n",
    "                                                                     learning_rate = 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Tuning: hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UFq5wbhuHfZQ"
   },
   "outputs": [],
   "source": [
    "#Tuning hidden size\n",
    "\n",
    "import json \n",
    "\n",
    "hidden_sizes = [50, 100, 200, 300, 500]\n",
    "\n",
    "hidden_results = {}\n",
    "\n",
    "for h in hidden_sizes:\n",
    "    \n",
    "    gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "                hidden_size=h, num_layers=1, num_classes=3, \n",
    "                vocab_size=len(id2token))\n",
    "\n",
    "    gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, gru_model, \n",
    "                                                                          num_epochs=10, learning_rate = 3e-4)\n",
    "    \n",
    "    hidden_results[h] = [gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc]\n",
    "    \n",
    "    #Save results\n",
    "    with open('/content/gdrive/My Drive/mnli/gru_hidden500.txt', 'w') as f:\n",
    "        f.write(json.dumps(hidden_results))\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Tuning: weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "OQbujd9uwlFU"
   },
   "outputs": [],
   "source": [
    "#Tuning Weight decay\n",
    "\n",
    "decay_results = {}\n",
    "\n",
    "decays = [1e-5, 1e-3, 1e-1, 0]\n",
    "\n",
    "for d in decays:\n",
    "    \n",
    "    gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "                hidden_size=200, num_layers=1, num_classes=3, \n",
    "                vocab_size=len(id2token))\n",
    "\n",
    "    gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, gru_model, \n",
    "                                                                          num_epochs=10, learning_rate = 3e-4,\n",
    "                                                                         decay = d)\n",
    "    \n",
    "    decay_results[d] = [gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc]\n",
    "    \n",
    "    with open('/content/gdrive/My Drive/mnli/gru_decay.txt', 'w') as f:\n",
    "        f.write(json.dumps(decay_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU Tuning: learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2057
    },
    "colab_type": "code",
    "id": "ZIYYG1WfeABc",
    "outputId": "1954d47c-b8c5-4982-876b-0b838d583a52",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tuning learning rate\n",
    "\n",
    "learning_rates = [3e-5, 3e-4, 3e-3, 3e-2]\n",
    "\n",
    "learning_results = {}\n",
    "\n",
    "for l in learning_rates:\n",
    "    \n",
    "    gru_model = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "                hidden_size=200, num_layers=1, num_classes=3, \n",
    "                vocab_size=len(id2token))\n",
    "\n",
    "    gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, gru_model, \n",
    "                                                                          num_epochs=10, learning_rate = l,\n",
    "                                                                         decay = 0)\n",
    "    \n",
    "    learning_results[l] = [gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc]\n",
    "    \n",
    "    with open('/content/gdrive/My Drive/mnli/gru_learning.txt', 'w') as f:\n",
    "         f.write(json.dumps(learning_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1IwoVpwQeABh"
   },
   "source": [
    "### Now lets implement 2-layer Convolutional Neural Net model for text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pmvPfOtneABj"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_weights, emb_size, hidden_size, num_layers, num_classes, vocab_size,\n",
    "                kernel_size=3, dropout=0):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embedding =  nn.Embedding(vocab_size, emb_size, \n",
    "                                       padding_idx=PAD_IDX).from_pretrained(emb_weights, \n",
    "                                                                freeze=True).to(device) #load preset\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=1).to(device)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=1).to(device)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size*2, hidden_size).to(device) #2 for concatenated\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes).to(device) # 2 for concatenated\n",
    "    \n",
    "     \n",
    "\n",
    "    def forward(self, x1, x2, len1, len2):\n",
    "        \n",
    "        batch_size, seq_len = x1.size()\n",
    "            \n",
    "        embed1 = self.embedding(x1.to(device))\n",
    "        hidden = self.conv1(embed1.float().transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        #max pool over time\n",
    "        hidden1 = torch.max(hidden, dim=1)[0]\n",
    "        \n",
    "        #Second sentence pass\n",
    "        \n",
    "        embed2 = self.embedding(x2.to(device))\n",
    "        hidden2 = self.conv1(embed2.float().transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden2 = self.conv2(hidden2.transpose(1,2)).transpose(1,2)\n",
    "        hidden2 = F.relu(hidden2.contiguous().view(-1, hidden2.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        #max pool over time\n",
    "        hidden2 = torch.max(hidden2, dim=1)[0]\n",
    "        \n",
    "        #Concatenate two vectors\n",
    "        combined_vector = torch.cat([hidden1, hidden2], dim=1)\n",
    "        \n",
    "        logits1 = self.linear1(combined_vector)\n",
    "        logits2 = self.linear2(logits1)\n",
    "        \n",
    "        return logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CdnUV56HeABo"
   },
   "outputs": [],
   "source": [
    "#CNN\n",
    "cnn_model = CNN(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=2, num_classes=3, \n",
    "            vocab_size=len(id2token))\n",
    "\n",
    "cnn_losses, cnn_accuracies = train(train_loader, val_loader, gru_model, num_epochs=50, learning_rate = 3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Tuning: Hidden Dimension Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2567
    },
    "colab_type": "code",
    "id": "6WK3KjPIeABu",
    "outputId": "ef6e56a3-0914-48b5-a394-8d57072d7d70"
   },
   "outputs": [],
   "source": [
    "#CNN Tuning: hidden size\n",
    "hidden_sizes = [50, 100, 200, 300, 500]\n",
    "\n",
    "hidden_results = {}\n",
    "\n",
    "for d in hidden_sizes:\n",
    "    \n",
    "    cnn_model = CNN(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=d, num_layers=2, num_classes=3, \n",
    "            vocab_size=len(id2token), kernel_size=3)\n",
    "\n",
    "    cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, cnn_model, \n",
    "                                                                          num_epochs=10, learning_rate = 3e-4)\n",
    "    \n",
    "    hidden_results[d] = [cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc]\n",
    "    \n",
    "    with open('/content/gdrive/My Drive/mnli/cnn_hidden.txt', 'w') as f:\n",
    "        f.write(json.dumps(hidden_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Tuning: weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2567
    },
    "colab_type": "code",
    "id": "H7kj5ztKvy_J",
    "outputId": "dce4ac9f-3f77-4250-8594-f4b99484492e"
   },
   "outputs": [],
   "source": [
    "#CNN Tuning: regularization\n",
    "decay_results = {}\n",
    "\n",
    "decays = [0, 1e-7, 1e-5, 1e-3, 1e-1]\n",
    "\n",
    "for d in decays:\n",
    "    \n",
    "    cnn_model = CNN(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=2, num_classes=3, \n",
    "            vocab_size=len(id2token), kernel_size=3)\n",
    "\n",
    "    cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, cnn_model, \n",
    "                                                                          num_epochs=10, learning_rate = 3e-4,\n",
    "                                                                         decay = d)\n",
    "    \n",
    "    decay_results[d] = [cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc]\n",
    "    \n",
    "    with open('/content/gdrive/My Drive/mnli/cnn_decay.txt', 'w') as f:\n",
    "        f.write(json.dumps(decay_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Tuning: dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1700
    },
    "colab_type": "code",
    "id": "f0ghZWcemaJO",
    "outputId": "1c923c4a-fbcf-4743-afcc-c8a88a67d883"
   },
   "outputs": [],
   "source": [
    "#CNN Tuning: dropout\n",
    "dropout_results = {}\n",
    "\n",
    "dropouts = [0, 0.25, 0.5, 0.75]\n",
    "\n",
    "for d in dropouts:\n",
    "    \n",
    "    cnn_model = CNN(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=2, num_classes=3, \n",
    "            vocab_size=len(id2token), dropout=d)\n",
    "\n",
    "    cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc  = train(train_loader, \n",
    "                                                                          val_loader, cnn_model, \n",
    "                                                                          num_epochs=10, learning_rate = 3e-4,\n",
    "                                                                         decay = 0)\n",
    "    \n",
    "    dropout_results[d] = [cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc]\n",
    "    \n",
    "    with open('/content/gdrive/My Drive/mnli/cnn_dropout.txt', 'w') as f:\n",
    "      f.write(json.dumps(dropout_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "L7zayX-huTmv",
    "outputId": "6df1eb20-96e1-44a0-b0fc-128b00a7b03f"
   },
   "outputs": [],
   "source": [
    "#Best model: CNN with weight decay 0.001\n",
    "cnn_model_best = CNN(emb_weights, emb_size=emb_weights.shape[1], \n",
    "        hidden_size=200, num_layers=2, num_classes=3, \n",
    "        vocab_size=len(id2token))\n",
    "\n",
    "cnn_train_losses, cnn_train_acc, cnn_val_losses, cnn_val_acc  = train(train_loader, \n",
    "                                                                      val_loader, cnn_model_best, \n",
    "                                                                      num_epochs=10, learning_rate = 3e-4,\n",
    "                                                                     decay= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "a5zRMSYJ-qQo",
    "outputId": "8959b93e-a760-4105-dc39-e716ac2e9182"
   },
   "outputs": [],
   "source": [
    "#Evaluate on validation set\n",
    "test_model(val_loader, cnn_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "9KYRTiCD-Qq7",
    "outputId": "f2022790-e078-415c-899b-1e077418282d"
   },
   "outputs": [],
   "source": [
    "#Best model: GRU with hidden size 200\n",
    "gru_model_best = GRU(emb_weights, emb_size=emb_weights.shape[1], \n",
    "            hidden_size=200, num_layers=1, num_classes=3, \n",
    "            vocab_size=len(id2token))\n",
    "\n",
    "gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(train_loader, \n",
    "                                                                      val_loader, gru_model_best, \n",
    "                                                                      num_epochs=10, learning_rate = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2KUCHkEX-4j9",
    "outputId": "0596a172-8218-4de3-9631-8177d1062df7"
   },
   "outputs": [],
   "source": [
    "#Evaluate on validation set\n",
    "test_model(val_loader, gru_model_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 correct and 3 incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MPNAZpcmyT1s"
   },
   "outputs": [],
   "source": [
    "#Output 3 correct and 3 incorrect predictions\n",
    "\n",
    "#Create loader that doesn't shuffle input so we can recover actual text\n",
    "val_loader_unshuffled = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "def output_correct_incorrect(loader, model):\n",
    "  \n",
    "    '''Output indices of 3 correct and 3 incorrect predictions'''\n",
    "    \n",
    "    predictions = []\n",
    "  \n",
    "    correct_ind = []\n",
    "    for sentence1, sentence2, lengths1, lengths2, labels in loader:\n",
    "\n",
    "        outputs = F.softmax(model(sentence1, sentence2, lengths1, lengths2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]#.to(device)\n",
    "\n",
    "        #Find correct and incorrect sentences\n",
    "        pred = predicted.view(predicted.numel()).cpu().numpy()\n",
    "        predictions.extend(pred)\n",
    "        labs = labels.numpy()\n",
    "        ind_correct = np.equal(pred,labs)\n",
    "        correct_ind.extend(ind_correct)\n",
    "\n",
    "\n",
    "    corrects = [i for i, x in enumerate(correct_ind) if x]\n",
    "    incorrect = [i for i, x in enumerate(correct_ind) if not x]\n",
    "    \n",
    "    correct_ind = random.sample(corrects, 3)\n",
    "    incorrect_ind = random.sample(incorrect, 3)\n",
    "    \n",
    "    return correct_ind, incorrect_ind, predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lCD8QagtHk3w"
   },
   "outputs": [],
   "source": [
    "#3 correct and 3 incorrect predictions by the best CNN model\n",
    "correct_ind, incorrect_ind, predictions = output_correct_incorrect(val_loader_unshuffled, cnn_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "9gyVRFxnIrJw",
    "outputId": "5937e125-4cb7-4aa0-b130-f21294fc365e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Predictions\n",
      "\n",
      "Prediction:  668\n",
      "['Person doing a trick on a skateboard in a skate park while people watch .'\n",
      " 'There is a person imitating a skateboard trick from Tony Hawk .'\n",
      " 'neutral']\n",
      "Predicted:  1\n",
      "\n",
      "Prediction:  989\n",
      "['A scientist with a checkered shirt on is looking into a microscope to learn more about the world .'\n",
      " 'A scientist is using a microscope .' 'entailment']\n",
      "Predicted:  2\n",
      "\n",
      "Prediction:  310\n",
      "['A blond woman wearing a blue and pink floral tie-front bikini on a beach , readying to put a flagpole in the sand .'\n",
      " 'A person in a bikini' 'entailment']\n",
      "Predicted:  2\n"
     ]
    }
   ],
   "source": [
    " print('Correct Predictions')\n",
    "\n",
    "for i in correct_ind:\n",
    "\n",
    "    print('\\nPrediction: ', str(i))\n",
    "    print(val_data.iloc[i][['sentence1', 'sentence2', 'label']].values)\n",
    "    print('Predicted: ', predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "vQOlED4ZKYJ4",
    "outputId": "527e116a-da87-478d-d7a8-5058b7e52057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Predictions\n",
      "\n",
      "Prediction:  382\n",
      "['A boy wearing a yellow jersey is accepting the baton from a female wearing a yellow jersey in a relay race .'\n",
      " 'A boy accepting a baton in a race .' 'entailment']\n",
      "Predicted:  1\n",
      "\n",
      "Prediction:  222\n",
      "['A woman in a blue shirt and black workout pants practicing martial arts in front of a house .'\n",
      " 'A woman has a white shirt .' 'contradiction']\n",
      "Predicted:  2\n",
      "\n",
      "Prediction:  993\n",
      "['An old bearded man plays a hand flute on the side of a sidewalk .'\n",
      " 'An old bearded man plays a board game on the side of a sidewalk .'\n",
      " 'contradiction']\n",
      "Predicted:  1\n"
     ]
    }
   ],
   "source": [
    "#0=contradiction, 1=neutral, 2=entailment\n",
    "print('Incorrect Predictions')\n",
    "\n",
    "for i in incorrect_ind:\n",
    "\n",
    "    print('\\nPrediction: ', str(i))\n",
    "    print(val_data.iloc[i][['sentence1', 'sentence2', 'label']].values)\n",
    "    print('Predicted: ', predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance of the best models on MNLI by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cq2f8nxz3U7M"
   },
   "outputs": [],
   "source": [
    "#Test performance on NMLI by genre\n",
    "mnli_data = read_data('gdrive/My Drive/mnli/hw2_data.nosync/mnli_val.tsv', sep='\\t')\n",
    "mnli_train = read_data('gdrive/My Drive/mnli/hw2_data.nosync/mnli_train.tsv', sep='\\t')\n",
    "\n",
    "#Tokenize\n",
    "mnli_data = tokenize(mnli_data)\n",
    "mnli_train = tokenize(mnli_train)\n",
    "\n",
    "\n",
    "#Assign label\n",
    "mnli_data['target'] = mnli_data.label.apply(lambda x: assign_target(x))\n",
    "mnli_train['target'] = mnli_train.label.apply(lambda x: assign_target(x))\n",
    "\n",
    "#Save genres\n",
    "genres = mnli_data.genre.unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NSHcp2DQ7PKf"
   },
   "outputs": [],
   "source": [
    "#Calculate accuracy by genre\n",
    "acc_by_genre_cnn = {}\n",
    "\n",
    "for genre in genres:\n",
    "\n",
    "  #Create loaders\n",
    "    mnli_genre = mnli_data[mnli_data.genre == genre]\n",
    "    mnli_dataset = VocabDataset(zip(mnli_genre.input1,mnli_genre.input2, \n",
    "                                             mnli_genre.target), token2id)\n",
    "\n",
    "    mnli_loader = torch.utils.data.DataLoader(dataset=mnli_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=vocab_collate_func,\n",
    "                                             shuffle=False)\n",
    "  \n",
    "  \n",
    "  \n",
    "    accuracy = test_model(mnli_loader, cnn_model_best)\n",
    "    acc_by_genre_cnn[genre] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "p_pKWSizRDYe",
    "outputId": "e11c3cfd-9afe-4c95-f77d-f6fa5602a148"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': 42.814070351758794,\n",
       " 'government': 44.19291338582677,\n",
       " 'slate': 40.818363273453095,\n",
       " 'telephone': 43.28358208955224,\n",
       " 'travel': 45.010183299389}"
      ]
     },
     "execution_count": 121,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy by CNN model\n",
    "acc_by_genre_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "b3SsGCRZ_Ffo"
   },
   "outputs": [],
   "source": [
    "#Repeat with the best GRU model\n",
    "acc_by_genre_gru = {}\n",
    "\n",
    "for genre in genres:\n",
    "\n",
    "  #Create loaders\n",
    "    mnli_genre = mnli_data[mnli_data.genre == genre]\n",
    "    mnli_dataset = VocabDataset(zip(mnli_genre.input1,mnli_genre.input2, \n",
    "                                             mnli_genre.target), token2id)\n",
    "\n",
    "    mnli_loader = torch.utils.data.DataLoader(dataset=mnli_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=vocab_collate_func,\n",
    "                                             shuffle=False)\n",
    "  \n",
    "    accuracy = test_model(mnli_loader, gru_model_best)\n",
    "    acc_by_genre_gru[genre] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "AshJDVcqRIjr",
    "outputId": "06be3c0d-149c-4830-b7bb-ee9e8a4b37d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': 41.70854271356784,\n",
       " 'government': 40.15748031496063,\n",
       " 'slate': 39.920159680638726,\n",
       " 'telephone': 40.995024875621894,\n",
       " 'travel': 41.34419551934827}"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy by GRU model\n",
    "acc_by_genre_gru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: additional training on MNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "7iRoupLFRKzs",
    "outputId": "76bda7b9-30d5-4a6a-c01b-7a0e9049cac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction\n",
      "44.92462311557789\n",
      "telephone\n",
      "47.56218905472637\n",
      "slate\n",
      "44.510978043912175\n",
      "government\n",
      "47.83464566929134\n",
      "travel\n",
      "44.5010183299389\n"
     ]
    }
   ],
   "source": [
    "#Optional: fine-tuning on MNLI data\n",
    "best_model = cnn_model_best\n",
    "torch.save(best_model.state_dict(), 'gdrive/My Drive/mnli/best_model.pt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc_by_genre_extra = {}\n",
    "\n",
    "for genre in genres:\n",
    "  \n",
    "    print(genre)\n",
    "  \n",
    "    #Create loaders\n",
    "    mnli_genre = mnli_data[mnli_data.genre == genre]\n",
    "    mnli_dataset = VocabDataset(zip(mnli_genre.input1,mnli_genre.input2, \n",
    "                                              mnli_genre.target), token2id)\n",
    "\n",
    "    mnli_loader = torch.utils.data.DataLoader(dataset=mnli_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=vocab_collate_func,\n",
    "                                             shuffle=False)\n",
    "  \n",
    "  \n",
    "    mnli_train_dataset = mnli_train[mnli_train.genre == genre]\n",
    "    mnli_train_dataset = VocabDataset(zip(mnli_train_dataset.input1,mnli_train.input2, \n",
    "                                             mnli_train.target), token2id)\n",
    "\n",
    "    mnli_train_loader = torch.utils.data.DataLoader(dataset=mnli_train_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             collate_fn=vocab_collate_func,\n",
    "                                             shuffle=True)\n",
    "  \n",
    "    best_model.load_state_dict(torch.load('gdrive/My Drive/mnli/best_model.pt'))\n",
    "  \n",
    "    gru_train_losses, gru_train_acc, gru_val_losses, gru_val_acc  = train(mnli_train_loader, \n",
    "                                                                      mnli_loader, best_model, \n",
    "                                                                      num_epochs=5, learning_rate = 3e-4, \n",
    "                                                                     decay = 0.001)\n",
    "  \n",
    "    accuracy = test_model(mnli_loader, best_model)\n",
    "    print(accuracy)\n",
    "    acc_by_genre_extra[genre] = accuracy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_modular.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
